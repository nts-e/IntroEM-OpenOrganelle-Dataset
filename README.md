# OpenOrganelle

Welcome to the Electrom Microscope OpenOrganelle dataset README! (Following this excellent [paper](https://www.nature.com/articles/s41586-021-03977-3)) 
This repository serves as a practical entry point for developers aiming to understand and work with the dataset. The "hello_data" Jupyter notebook, included here, demonstrates the basic process of loading a raw image crop from the dataset using MongoDB metadata and provides access to the corresponding ground truth annotated image. The README also offers detailed explanations of dataset terminology and tools for viewing images, along with insights into the file hierarchy for easy navigation. For those eager to get hands-on with the data, the README provides clear instructions on how to download the dataset, ensuring a smooth start to your exploration. 

#### Paper

1.  [Preprint](https://www.biorxiv.org/content/10.1101/2020.11.14.382143v1.full), [Nature](https://www.nature.com/articles/s41586-021-03977-3).

#### Format and contents

 -  The datasets are stored in the N5 file format , with similar structure to HDF5 file:
	 - Large multi-dimensional numpy arrays
	 - Group hierarchy
	 - Metadata file
 - Each dataset is a single cell. Size is ~500GB per dataset.


#### Resolutions

- The original EM images have been down sampled by partitioning the source data into a collection of cubes of voxels with edge length 2N (N ranges from 1 to 5), and computing a reducing function over the values within each cube (mean or modal, depends on the context).
- Therefore, the numpy arrays are supplied with several resolutions (5-6), marked s0 to sN. s0 marks the highest resolution, s1 is half than s0, s2 is half than s1, etc.
- Dataset sizes (the examples below and later are taken from the jrc_macrophage-2 dataset):  
  
    ![sizes](https://github.com/nts-e/OpenOrganelle/blob/main/mdfiles/md_1.png)
- For s0, image voxels (cubes) are of size [nm]: 4.0 x 4.0 x 3.36 (X, Y, Z)
- The _attributes.json_ file inside the relevant N5 group contains the metadata describing the multiscale image representation:
![attributes](https://github.com/nts-e/OpenOrganelle/blob/main/mdfiles/md_2.jpg)
#### Data Access

- Datasets can be accessed in several methods: AWS CLI, FIJI tool, Python.
- Using a free AWS account, I could access the data:  
    ![s3](https://github.com/nts-e/OpenOrganelle/blob/main/mdfiles/md_3.png)
- The data is stored in two alternative s3 buckets (structure comparison in mdfiles/md_4.png):

		A) _s3://janelia-cosem/_ - The data which is pointed from the OpenOrganelle site.

		B) _s3://janelia-cosem-publications/heinrich-2021a/_ - The data used for the paper.

- What I downloaded are the datasets fron bucket B ( I chose B and not A, because the provided code in GitHub refers to B. Note that I could not explain why the size of the raw data in A is not equal to its size in B).

- For the internal lab users, the datasets are stored in this location: [\\132.72.65.219\qtree_assafzar\assafzar\OpenOrganelle\janelia-cosem-publications\heinrich-2021a](file://132.72.65.219/qtree_assafzar/assafzar/OpenOrganelle/janelia-cosem-publications/heinrich-2021a)

- I recommend beginning from **jrc_hela-2**.  

- I wrote a python procedure to download the data (todo: add procedure).

- A "hello world" for loading images from the datasets is provided [here](https://github.com/nts-e/OpenOrganelle/blob/main/Code/data_access/hello_data.ipynb).  

#### Dataset terminology
* gt = Ground Truth – human annotated organelles.
* Predictions – soft segmentation of the FIB-SEM data for a particular object class, created by ML models trained on the ground truth, normalized to [0, 255] (high at a location inside an instance of an object class and low at a location outside an instance of an object class, 127 is 0 distance).
* Distance – TANH signed distance transform. A nice [explanation video](https://www.youtube.com/watch?v=oxWfLTQoC5A) about Distance Transform.
* Mask – used to roughly separate sample material from resin.
* Segmentations - suggested segmentations of the FIB-SEM data for a particular object class. Generated by post-processing the prediction volumes. They used a bunch of postprocessing techniques. The techniques are reviewed in this [document](https://github.com/nts-e/OpenOrganelle/blob/main/mdfiles/Refinements%20in%20the%20paper-EM%20data.docx) (credits to Esraa).

#### Tools

- The images can be viewed using this nice site: [https://openorganelle.janelia.org/](https://openorganelle.janelia.org/)  
      
    for example, the jrc_hela-2 is here: [https://openorganelle.janelia.org/datasets/jrc_hela-2](https://openorganelle.janelia.org/datasets/jrc_hela-2)  
      
    Setup your choices and then click on the **View** button. It opens a light-weight viewer showing the image.  
- The datasets file hierarchy and a detailed readme file can be viewed here: https://open.quiltdata.com/b/janelia-cosem-publications/tree/heinrich-2021a/
  
   
#### Code

- The authors provide [this](https://github.com/saalfeldlab/CNNectome) code which has been used to build, train, predict, segment, and evaluate the data.
- Unfortunately, the code is not documented.
- There is a massive use of the [Gunpowder](http://funkey.science/gunpowder/index.html) library. Gunpowder allows you to assemble a pipeline on arbitrarily large volumes of multi-dimensional images. It complements PyTorch/TensorFlow.  
    I found it cumbersome and difficult. You might try different approaches.
    I followed [this](http://funkey.science/gunpowder/tutorial_simple_pipeline.html) tutorial to understand the Gunpowder concepts. It helped to shed some light on the authors’ code.
- I lightly documented the train and infer procedures of the authors (lab users: all is [here](file://132.72.65.219/qtree_assafzar/assafzar/Nitsan/OpenOrganelle/partial_training/setup37) , training is the train_setup_37.ipynb file).
- The code supports TF1. I could run it successfully, but the training was very very long and we could not achieve the same results (after several weeks we’ve only reached ~320K iterations of the 500K which were recommended.). There are [many networks and setups variations](https://github.com/janelia-cellmap/training_setups/blob/master/v0003.2/summary.md) that are provided. The best networks and setups are reported per organelle, dataset, and iteration.
- Lab users: trained model files from the authors can be found [here](file://132.72.65.219/qtree_assafzar/assafzar/Nitsan/OpenOrganelle/partial_training/trained_models_from_janelia).
- We tried several days to convert these files to TF2 but it did not work. 


#### Model Training
 - The paper's authors trained models by taking several cubes from each dataset as training sets and then validate/test against other cubes from the same dataset or from other datasets.
 - In the paper, there are up to 35 organelle classes which are annotated, in 28 training blocks from five datasets covering four different cell types. 
 - Datasets used in the paper: jrc_hela-2, jrc_hela-3, jrc_jurkat-1 and jrc_macrophage-2.
